import io
import pandas as pd
import chardet
from fastapi import UploadFile, HTTPException
from typing import Mapping, List, Tuple, Callable, Any, Dict
from openpyxl import load_workbook

# 1) Config centralizada de lectura seg√∫n keyword
EXCEL_READ_CONFIGS: Dict[str, Dict[str, Any]] = {
    "people_consultation": {},  # default: lee hoja por defecto
    "scheduling_ppp":  {
        "sheet_name": "RESUMEN",
        "header": 2,
        "skiprows": 3,
        "engine": "openpyxl",
    },
    "schedule_ppp":  {
        "sheet_name": "RESUMEN",
        "skiprows": 4,
        "header": [0, 1],
        "engine": "openpyxl",
    },
    "api_id": {},      # si necesita algo especial, lo pones aqu√≠
    "master_concentrix": {
        "sheet_name": "AGENTES_UBY",
        "engine": "openpyxl",
    },
    "master_ubycall": {
        "sheet_name": "AGENTES_GLOVO",
        "engine": "openpyxl",
    },
    "scheduling_ubycall": {},   # idem
    "planned_data":    {"sheet_name": "DDPP", "engine": "openpyxl"},
}

CSV_READ_CONFIGS = {
    "contacts_with_ccr": {
        "engine": "python",
        "drop_last_row": True,
    }
}


def read_file_safely(file_stream: io.BytesIO, filename: str) -> pd.DataFrame:
    """
    Lee un archivo Excel (.xlsx) o CSV (.csv) con detecci√≥n de codificaci√≥n.
    Aplica configuraciones espec√≠ficas definidas en EXCEL_READ_CONFIGS.
    """
    file_stream.seek(0)
    lower = filename.lower()

    # === Excel ===
    if filename.endswith('.xlsx'):

        # 2) Buscar la primera keyword en el nombre
        for kw, params in EXCEL_READ_CONFIGS.items():
            if kw in lower:
                try:
                    return pd.read_excel(file_stream, **params)
                except Exception as e:
                    raise ValueError(
                        f"Error leyendo Excel '{filename}' (config '{kw}'): {e}")
        # 3) Default si no hubo match
        try:
            return pd.read_excel(file_stream, engine='openpyxl')
        except Exception as e:
            raise ValueError(
                f"Error leyendo Excel '{filename}' (default): {e}")

    # === CSV ===
    elif filename.endswith('.csv'):
        try:
            raw = file_stream.read()
            encoding = chardet.detect(raw).get("encoding", "utf-8")
            file_stream.seek(0)

            text = raw.decode(encoding, errors="ignore")

            # üî• eliminar √∫ltima l√≠nea corrupta ANTES del parseo
            lines = text.splitlines()
            if len(lines) > 1:
                text = "\n".join(lines[:-1])

            cleaned_stream = io.StringIO(text)

            # buscar config
            csv_config = None
            for kw, cfg in CSV_READ_CONFIGS.items():
                if kw in lower:
                    csv_config = cfg
                    break

            df = pd.read_csv(
                cleaned_stream,
                engine=csv_config.get("engine") if csv_config else "python",
                sep=None,               # autodetecta
                on_bad_lines="skip",    # extra seguridad
            )

            return df
        except Exception as e:
            raise ValueError(f"Error leyendo CSV '{filename}': {e}")

    else:
        raise ValueError(f"Formato no soportado: '{filename}'")


async def handle_file_upload_generic(
    files: List[UploadFile],
    validator: Callable[[str], str],
    keyword_to_slot: Mapping[str, str],
    required_slots: List[str],
    post_process: Callable[..., Any]
) -> Any:
    """
    - files: lista de UploadFile
    - validator: funci√≥n file_name ‚Üí validated_name
    - keyword_to_slot: dict donde clave es keyword en filename validado, valor es nombre de atributo para guardar el DataFrame
    - required_slots: lista de slots obligatorios
    - post_process: funci√≥n que recibe los DataFrames en orden de required_slots + opcionales, y retorna el resultado final
    """

    # Inicializar slots en None
    slot_data = {slot: None for slot in keyword_to_slot.values()}

    # Procesar cada archivo
    for file in files:
        safe_name = validator(file.filename)
        content = await file.read()
        df = read_file_safely(io.BytesIO(content), safe_name)

        # Buscar a qu√© slot va
        lower = safe_name.lower()
        for kw, slot in keyword_to_slot.items():
            if kw in safe_name.lower():
                slot_data[slot] = df
                break

    # Verificar faltantes
    missing = [s for s in required_slots if slot_data.get(s) is None]
    if missing:
        raise ValueError(f"Faltan archivos requeridos: {', '.join(missing)}")

    # Llamar al post-proceso con los DataFrames en el orden de required_slots,
    # y adem√°s pasar todo slot_data si el post_process lo necesita.
    dfs_required = [slot_data[s] for s in required_slots]

    optional_slots = {
        slot: df
        for slot, df in slot_data.items()
        if slot not in required_slots
    }
    return post_process(*dfs_required, **optional_slots)
